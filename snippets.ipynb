{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe2af8ca-64aa-4dd7-9c7a-67b3b6d2def0",
   "metadata": {},
   "source": [
    "### Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf1c09-88ca-402b-9c43-3ffd67a4e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the dataframe:\n",
    "df = pd.read_csv('DiabetesDataset.csv')\n",
    "\n",
    "## Show statistics:\n",
    "df.info(), df.describe(), df.types, df.head(), df.columns()\n",
    "\n",
    "## Selecting certain columns:\n",
    "df.iloc[:,1:4] or df[\"col1\",\"col2\"]\n",
    "\n",
    "## Drop NaN values:\n",
    "print(df.shape)\n",
    "print(data.isnull().sum())\n",
    "data = data.dropna(axis=0, inplace=True).reset_index(drop=True)\n",
    "\n",
    "## Drop columns:\n",
    "data.drop(['Cabin', 'Name', 'PassengerId', 'Ticket'], axis=1, inplace=True)\n",
    "\n",
    "## Fill NaN values (all or columns):\n",
    "df.fillna(130, inplace = True)\n",
    "df[\"Calories\"].fillna(130, inplace = True)\n",
    "x = df[\"Calories\"].mean()\n",
    "x = df[\"Calories\"].median()\n",
    "df[\"Calories\"].fillna(x, inplace = True)\n",
    "\n",
    "## Replace values:\n",
    "pima2.replace(0, np.nan,inplace=True)\n",
    "\n",
    "## Remove duplicates:\n",
    "print(df.duplicated())\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "## Change values in dataframe:\n",
    "df['Duration'] = df['Duration'].replace({60:'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\n",
    "\n",
    "## Change the date:\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "## Selecting X and Y:\n",
    "X = df.drop(columns=['Diabetes'], axis=1)\n",
    "y = df['Diabetes'].values\n",
    "\n",
    "## Train-Test Split: (Train-Test Dataset): Select correct split amount:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "print (\"Training Set Size:\", len(X_train))\n",
    "print (\"Test Set Size:\", len(X_test))\n",
    "\n",
    "## Create a Validation Dataset:\n",
    "X_train_val, X_val, y_train_val, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "print (\"Training Set Size:\", len(X_train_val))\n",
    "print (\"Validation Set Size:\", len(X_val))\n",
    "\n",
    "## Polynomial and Log-Transform:\n",
    "# Polynomial transform\n",
    "poly = PolynomialFeatures(degree=2)  # Change degree as per requirements\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "print(\"Original X_train shape: \", X_train.shape)\n",
    "print(\"Transformed X_train shape: \", X_train_poly.shape)\n",
    "# Log Transform:\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "X_train_log = transformer.transform(X_train)\n",
    "X_test_log = transformer.transform(X_test)\n",
    "print(\"Log-transformed X_train shape: \", X_train_log.shape)\n",
    "\n",
    "## Use StandardScaler or PowerTransformer or MinMaxScaler:\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler\n",
    "scaler = PowerTransformer()\n",
    "Xtrain_scale = scaler.fit_transform(X_train)\n",
    "Xtest_scale = scaler.transform(X_test)\n",
    "# Transform and show histogram:\n",
    "Xtrain = pd.DataFrame(Xtrain_scale, columns=X.columns)\n",
    "Xtrain.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n",
    "\n",
    "## One-Hot Encoding of a categorical Feature:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot = OneHotEncoder()\n",
    "one_hot_degree = one_hot.fit_transform(data[[\"Degree\"]]).toarray()\n",
    "one_hot_degree = pd.DataFrame(one_hot_degree, columns=one_hot.get_feature_names_out())\n",
    "data_tree = pd.concat([data, one_hot_degree], axis=1)\n",
    "\n",
    "## Ordinal encoding (i.e. Male&Female -> 0,1) Categorical Features:\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "oe=OrdinalEncoder()\n",
    "# select variables for label encoding\n",
    "categorical_cols=['Sex', 'BP', 'Cholesterol', 'Drug']\n",
    "# set up your preprocessor (name, transformer, columns to transform)\n",
    "preprocessor = ColumnTransformer([('categorical', oe, categorical_cols)], remainder='passthrough')\n",
    "encoded_df = pd.DataFrame(preprocessor.fit_transform(df), columns=['Sex', 'BP', 'Cholesterol', 'Drug', 'Age', 'Na_to_K'])\n",
    "\n",
    "## Training Loop:\n",
    "\n",
    "models =[(\"RF\", RandomForestClassifier()),\n",
    "         (\"kNN\", KNeighborsClassifier(n_neighbors=3)) ]\n",
    "results = []\n",
    "names = []\n",
    "finalResults = []\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    model_results = model.predict(X_test)\n",
    "    train_score = precision_score(y_train, model.predict(X_train), average='macro')\n",
    "    print(train_score)\n",
    "    test_score = precision_score(y_test, model_results, average='macro')\n",
    "    print(test_score)\n",
    "    results.append(test_score)\n",
    "    names.append(name)\n",
    "    finalResults.append((name, test_score, train_score))\n",
    "    print(model)\n",
    "    print(model.classes_)\n",
    "    cm = confusion_matrix(y_test, model_results, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "finalResults.sort(key=lambda k:k[1],reverse=True)\n",
    "\n",
    "## Grid Search CV:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf=RandomForestClassifier()\n",
    "params= {'n_estimators':[10,50,100,200],\n",
    "         'max_depth':list(range(1,7))}\n",
    "estimator= GridSearchCV(rf, params,cv=10, scoring='f1_macro', n_jobs=-1)\n",
    "estimator.fit(X_train, y_train)\n",
    "RF_par=estimator.best_params_\n",
    "\n",
    "## Permutation Importance:\n",
    "from sklearn.inspection import permutation_importance\n",
    "best_rf_model = estimator.best_estimator_\n",
    "perm_importances = permutation_importance(\n",
    "    best_rf_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
    "forest_importances = pd.Series(perm_importances.importances_mean, index=features)\n",
    "sort_index = np.argsort(forest_importances)[::-1]\n",
    "# plot the importances\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances[sort_index].plot.bar(yerr=perm_importances.importances_std[sort_index], ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## PERFORMANCE MEASUREMENTS\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,average_precision_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "## Confusion Matrix:\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "labels = []\n",
    "sns.heatmap(conf, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            cbar=False, \n",
    "            cmap=\"coolwarm_r\", \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels, \n",
    "            linewidth = 1)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "## ROC and Precision-Recall:\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Calculates Probabilities & Keep the Probabilities of the positive class only:\n",
    "probs = model.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# Function for plotting the ROC curve\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label = 'random classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curve & Compute the Area Under the ROC Curve (AUC) - the ROC AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "plot_roc_curve(fpr, tpr)\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(\"AUC: \" , round(auc, 3))\n",
    "\n",
    "# Function for plotting the Precision-Recall curve\n",
    "def plot_rpc(recall, precision):\n",
    "    plt.plot(recall, precision, color='orange', label='RPC')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall = True Positive Rate')\n",
    "    plt.title('Recall-Precision Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot Precision-Recall curve  & Compute average precision - Precision-Recall AUC\n",
    "precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "plot_rpc(recall, precision)\n",
    "average_precision = average_precision_score(y_test, probs)\n",
    "print(\"Average Precision: \", round(average_precision, 3))\n",
    "\n",
    "\n",
    "## Correlation Matrix\n",
    "# Simple:\n",
    "corrmatrix=df.corr()\n",
    "import seaborn as sns\n",
    "corrmatrix\n",
    "sns.heatmap(corrmatrix)\n",
    "\n",
    "## Advanced correlation Matrix (absolute)\n",
    "# Get Feature names and column names\n",
    "features=df.iloc[:,:-1].columns;\n",
    "corr = df[features].corr()\n",
    "dcorr = corr.stack().reset_index()\n",
    "dcorr.columns = ['F1', 'F2', 'Corr']\n",
    "mask_dups = (dcorr[['F1', 'F2']].apply(frozenset, axis=1).duplicated()) | (dcorr['F1']==dcorr['F2'])\n",
    "dcorr = dcorr[~mask_dups]\n",
    "dcorr['Corr_abs'] = dcorr['Corr'].abs()\n",
    "# Highest absolute:\n",
    "dcorr.sort_values(by=['Corr_abs'], ascending=False).head(10)\n",
    "# Highest Positive:\n",
    "dcorr.sort_values(by=['Corr'], ascending=False).head(10)\n",
    "# Highest Negative:\n",
    "dcorr.sort_values(by=['Corr'], ascending=True).head(10)$\n",
    "# Plot it:\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "_ = sns.heatmap(corr, cmap=cmap, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60a43656-46cf-4840-9249-1ce919537766",
   "metadata": {},
   "source": [
    "## KNN \n",
    "* `n_neighbors`, default=5\n",
    "*  `weights`{‘uniform’, ‘distance’} default=’uniform’=All equal weight, distance=weigh by inverse distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0451bb3-f3ae-401e-b355-beadf48ae9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,average_precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## KNN normal\n",
    "model =  KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Accurracy, Recall, Precision & F1-Score:\n",
    "vacc = accuracy_score(y_test, pred)\n",
    "rec = recall_score(y_test, pred)\n",
    "prec = precision_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "\n",
    "## KNN with Validation dataset:\n",
    "scores = []\n",
    "for k in range(1,101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_val, y_train_val)\n",
    "    pred = knn.predict(X_val)\n",
    "    val_score = knn.score(X_val, y_val)\n",
    "    scores.append(acc)\n",
    "# Plot and find best k:\n",
    "best_k = np.argmax(scores)+1\n",
    "best_score = scores[best_k-1]\n",
    "plt.plot(range(1,101), scores)\n",
    "plt.title(\"best k at {} with score of {}\".format(best_k, round(best_score,3)))\n",
    "plt.axvline(x=best_k, c=\"k\",  ls=\"--\")\n",
    "plt.show()\n",
    "\n",
    "## KNN with Cross Validation (k-fold) 10 (cross-val-score)\n",
    "mean_scores = np.array([])\n",
    "scores_std =np.array([])\n",
    "for k in range(1,101):\n",
    "    knn_cross = KNeighborsClassifier(n_neighbors=k)\n",
    "    cv_scores = cross_val_score(knn_cross, X_train, y_train, cv=10)\n",
    "    mean_scores = np.append(mean_scores, np.mean(cv_scores))\n",
    "    scores_std = np.append(scores_std, np.std(cv_scores))\n",
    "# Plot & find best k:\n",
    "best_k = np.argmax(mean_scores)+1\n",
    "best_score = mean_scores[best_k-1]\n",
    "plt.plot(range(1,101), mean_scores)\n",
    "plt.title(\"best k at {} with score of {}\".format(best_k, round(best_score,3)))\n",
    "plt.fill_between(range(0, len(mean_scores)), mean_scores + scores_std, mean_scores - scores_std, alpha=0.15, color='blue')\n",
    "plt.axvline(x=best_k, c=\"k\",  ls=\"--\")\n",
    "plt.show()\n",
    "\n",
    "## KNN with Hyperparameter Tuning Grid Search Cross-Validation:\n",
    "grid = {'n_neighbors':np.arange(1,100),\n",
    "        'p':np.arange(1,3),\n",
    "        'weights':['uniform','distance']\n",
    "       }\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, grid, cv=10)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "print(\"Hyperparameters:\", knn_cv.best_params_)\n",
    "print(\"CV Mean Accuracy Score:\", round(knn_cv.best_score_, 4))\n",
    "# Best estimator\n",
    "knn_best = knn_cv.best_estimator_\n",
    "\n",
    "## Confusion Matrix:\n",
    "knn_cross = KNeighborsClassifier(n_neighbors=best_k).fit(X_train,y_train)\n",
    "pred = knn_cross.predict(X_test)\n",
    "acc = knn_cross.score(X_test,y_test)\n",
    "conf = confusion_matrix(y_test, pred)\n",
    "sns.heatmap(conf, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            cbar=False, \n",
    "            cmap=\"coolwarm_r\", \n",
    "            linewidth = 1)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b98ba7a-c144-40a8-8c69-7bebcf3e5aac",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20783dd6-6642-4437-b007-b3b7617ee2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports:\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "## Train a Decision Tree:\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "# Accurracy on training set:\n",
    "tree.score(X_train, y_train)\n",
    "\n",
    "## Plotting the trees:\n",
    "plt.figure(figsize=(12,12))\n",
    "plot_tree(tree, filled=True);\n",
    "tree.get_depth()\n",
    "\n",
    "## Plotting the Confusion Matrix:\n",
    "def accuracy_conf_mat(y_test, y_pred):\n",
    "  print(\"Accuracy score:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "  conf_mat = confusion_matrix(y_test, y_pred)\n",
    "  cm_display = ConfusionMatrixDisplay(conf_mat).plot()\n",
    "accuracy_conf_mat(y_test, y_pred)\n",
    "\n",
    "## Different Depths of Decision Trees:\n",
    "for depth in [2, 3, 4, 5]:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth).fit(X_train, y_train)\n",
    "    y_pred = tree.predict(X_test)\n",
    "    print(\"Depth: \" + str(depth))\n",
    "    print(round(accuracy_score(y_test, y_pred), 2))\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plot_tree(tree, filled=True)\n",
    "    plt.show()\n",
    "    print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "## Dummy Classifier as baseline:\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(None, y_train)\n",
    "baseline = dummy.score(None, y_test)\n",
    "baseline\n",
    "\n",
    "\n",
    "## Grid Search to find max Depth:\n",
    "grid = {'max_depth':np.arange(1,7)}\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv = GridSearchCV(tree, grid, cv=5)\n",
    "tree_cv.fit(X_train, y_train)\n",
    "plot_tree(tree_cv.best_estimator_, filled=True)\n",
    "print(\"Hyperparameters (best max_depth):\", tree_cv.best_params_)\n",
    "print(\"Training CV Accuracy Score:\", round(tree_cv.best_score_, 4))\n",
    "print(\"Test Accuracy Score:\", round(tree_cv.score(X_test, y_test), 4))\n",
    "tree_best = tree_cv.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce494d8f-7a28-4de4-b43b-d02240b27834",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c027d-443b-4a7c-9739-e84ddc8bf8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Train Random Forest:\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state = 42, oob_score=True)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_pred_train = rfc.predict(X_train)\n",
    "y_pred_test = rfc.predict(X_test)\n",
    "\n",
    "## Calculate performance measures:\n",
    "acc_test = accuracy_score(y_pred_test, y_test)\n",
    "acc_train = accuracy_score(y_pred_train, y_train)\n",
    "acc_oob = rfc.oob_score_\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(conf_mat).plot()\n",
    "\n",
    "## Random Forest with Grid Search:\n",
    "grid = {'n_estimators': np.arange(100,1000,100),\n",
    "        'criterion': ['gini','entropy']\n",
    "       }\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_cv = GridSearchCV(rf, grid, cv=5)\n",
    "rf_cv.fit(X_train,y_train)\n",
    "print(\"Hyperparameters:\", rf_cv.best_params_)\n",
    "print(\"Training CV Accuracy Score:\", rf_cv.best_score_)\n",
    "print(\"Test Accuracy Score:\", rf_cv.score(X_test,y_test))\n",
    "\n",
    "## Permutation Importance to compute Feature Importance Plot\n",
    "result = permutation_importance(\n",
    "    rf_cv.best_estimator_, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "# Adjust Feature names:\n",
    "forest_importances = pd.Series(result.importances_mean, index=cancer.feature_names)\n",
    "sort_index = np.argsort(forest_importances)[::-1]\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e521ed0-6f41-47e8-9130-126d448b1f52",
   "metadata": {},
   "source": [
    "## Support Vector Machines \n",
    "Parmaters in `sklearn import svm`\n",
    "* `kernel`: Can be `poly`, `linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’\n",
    "* `gamma` \n",
    "* `degree`: Is the degree of `poly` i.e. 2\n",
    "* `C`: regularisation paramer i.e. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56f4f5-17af-4076-a69c-a6e45b09489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports:\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,average_precision_score\n",
    "\n",
    "## Fit a Polynomial Kernel:\n",
    "model = svm.SVC(kernel='poly', degree=2,C=1,gamma=0.10)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "## Fit a Radial Basis Kernel:\n",
    "model=SVC(kernel='rbf', gamma=1, C=1)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred=model.predict(Xtest_scale)\n",
    "print('Accuracy: %f' % accuracy_score(ytest, ypred))\n",
    "\n",
    "## For loop over changing C values 10^4-10^4 in 5 steps\n",
    "gamma=0.001\n",
    "CList=np.logspace(0,4,5)\n",
    "for C in CList:\n",
    "    model = svm.SVC(kernel='rbf',C=C,gamma=gamma)\n",
    "    model.fit(x_train, y_train)\n",
    "    PlotDecisionBoundary(model, x_train.values,y_train)\n",
    "\n",
    "## SVM with Cross validation:\n",
    "C_range = np.logspace(-3, 3, 7)\n",
    "gamma_range = np.logspace(-3, 3, 7)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "model=SVC()\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(model, param_grid=param_grid, cv=cv,n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "ypred = grid.best_estimator_.predict(X_test_scaled)\n",
    "target_names = ['negative', 'positive']\n",
    "print(classification_report(ytest, ypred, target_names=target_names))\n",
    "print(confusion_matrix(ytest, ypred))\n",
    "\n",
    "## Advanced Parameter tunning:\n",
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10, 15, 100, 1000]},\n",
    "                    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "score = 'accuracy'\n",
    "clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                   scoring=score)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "results = clf.cv_results_\n",
    "for i in range(len(results[\"params\"])):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (results[\"mean_test_score\"][i], results[\"std_test_score\"][i] * 2, results[\"params\"][i]))\n",
    "\n",
    "## SVM with LeaveOneOut Cross Validation:\n",
    "from sklearn import model_selection\n",
    "C = 0.03\n",
    "svc = svm.SVC(kernel='linear', C=C)\n",
    "loo = model_selection.LeaveOneOut()\n",
    "res = [svc.fit(X[train], y[train]).score(X[test], y[test]) for train, test in loo.split(X)]\n",
    "# Averge accurracy:\n",
    "np.mean(res)\n",
    "# Same with a for loop \n",
    "Clist = np.logspace(-3,3,14)\n",
    "for C in Clist:\n",
    "    svc = svm.SVC(kernel='linear', C=C)\n",
    "    loo = model_selection.LeaveOneOut()\n",
    "    res = [svc.fit(X[train], y[train]).score(X[test], y[test]) for train, test in loo.split(X)]\n",
    "    print('C: %f \\t accuracy: %f' % (C,np.mean(res))) #The average accuracy\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7be8250f-e082-4e37-8424-3389179537e2",
   "metadata": {},
   "source": [
    "### Linear Regression & Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b908fa-291d-40dc-af37-4a3994653fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "## Simple Linear Regression:\n",
    "LR = LinearRegression(fit_intercept=True)\n",
    "LR.fit(X, y)\n",
    "# R2 score (a,b): \n",
    "print(\"params: \", LR.coef_)\n",
    "print(\"constant: \", LR.intercept_)\n",
    "print(\"R^2 score: \", LR.score(X, y))\n",
    "# Predictions:\n",
    "print(\"TV: \", 200, \"Radio: \", 50, \"Sales: \", LR.predict(np.array([200, 50]).reshape(-1,2)))\n",
    "print(\"TV: \", 200, \"Radio: \", 30, \"Sales: \", LR.predict(np.array([200, 30]).reshape(-1,2)))\n",
    "\n",
    "## Polynomial Features with LR:\n",
    "X = np.array(ad_df[[\"TV\", \"Radio\"]])\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly.shape\n",
    "\n",
    "## Different Degrees Polynomials: \n",
    "X = np.array(ad_df[[\"TV\"]])\n",
    "y = np.array(ad_df[\"Sales\"])\n",
    "train_err = []\n",
    "test_err = []\n",
    "for f in range(1,7):\n",
    "    poly = PolynomialFeatures(f)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=1)\n",
    "    LR = LinearRegression(fit_intercept=False)\n",
    "    LR.fit(X_train, y_train)\n",
    "    train_err.append(mean_squared_error(y_train, LR.predict(X_train)))\n",
    "    test_err.append(mean_squared_error(y_test, LR.predict(X_test)))\n",
    "# plot the training and test errors for the different models used    \n",
    "plt.plot(range(1,7), train_err, label=\"train_error\")\n",
    "plt.plot(range(1,7), test_err, label=\"test_error\")\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel(\"polynomial degree\")\n",
    "plt.ylabel(\"error\")\n",
    "\n",
    "## Ridge Regression:\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=1)\n",
    "test_err = []\n",
    "for alpha in np.linspace(0.1, 1, num=10):\n",
    "    ridge_regression = Ridge(alpha=alpha)\n",
    "    ridge_regression.fit(X_train, y_train)\n",
    "    test_err.append(mean_squared_error(y_test, ridge_regression.predict(X_test)))\n",
    "plt.semilogx(np.logspace(-4, 2, num=10), test_err)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"test error\");\n",
    "\n",
    "## Cross Validation with Ridge:\n",
    "ridge = RidgeCV(fit_intercept=False, cv=5)\n",
    "ridge.fit(X_train, y_train)\n",
    "# print he fitted regression coefficients\n",
    "ridge.coef_\n",
    "ridge.alpha_\n",
    "# fit the model with the best selected alpha\n",
    "ridge_best = Ridge(alpha=ridge.alpha_)\n",
    "ridge_best.fit(X_train, y_train)\n",
    "# compute the MSE on the test set\n",
    "mean_squared_error(y_test, ridge_best.predict(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "152c2379-5de8-469f-9262-028ab3ca7986",
   "metadata": {},
   "source": [
    "### Logistic Regression & Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b9db7-4de8-4314-a54d-9730b3ed675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), tokenizer=spacy_tokenizer)\n",
    "classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "pipe = Pipeline([('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(round(accuracy_score(y_test, y_pred), 4))\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat = ConfusionMatrixDisplay(conf_mat)\n",
    "conf_mat.plot()\n",
    "\n",
    "## Multinomial Logistic Regression:\n",
    "log_clf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42,max_iter=1000)\n",
    "log_clf.fit(X_train, y_train)\n",
    "y_pred = log_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "## Naive Bayes:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56886a2d-f084-42d3-9edd-317bfb3cb2de",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction PCA & Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579e5e0-341d-4db0-b3e3-40aacf6b0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "## Simple PCA fitting:\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "## Transform a dataset into a lower dim.\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(X)\n",
    "X_pca_reduced = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca_reduced.shape)\n",
    "\n",
    "## Visualise PCA: (Adjust accordingly)\n",
    "plt.scatter(X_pca_reduced[:,0], X_pca_reduced[:,1], c=df.child_mortality, s=df.gdp/100, cmap=\"jet\")\n",
    "\n",
    "## PCA with RandomForest (explained variance ratio of 95%)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "forest = RandomForestClassifier(random_state=42)\n",
    "forest.fit(X_train_reduced, y_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "y_pred = forest.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "## Inverse it and see the difference\n",
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');\n",
    "\n",
    "## Perform PCA with Naive Bayes: (Pipeline):\n",
    "unscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\n",
    "unscaled_clf.fit(X_train, y_train)\n",
    "pred_test = unscaled_clf.predict(X_test)\n",
    "print('\\nPrediction accuracy for the normal test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n",
    "\n",
    "## PCA + NB + Standardisation\n",
    "std_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\n",
    "std_clf.fit(X_train, y_train)\n",
    "pred_test_std = std_clf.predict(X_test)\n",
    "print('\\nPrediction accuracy for the normal test dataset with PCA')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n",
    "\n",
    "## Optional: Extract the Components from the Pipeline and scale:\n",
    "pca = unscaled_clf.named_steps['pca']\n",
    "X_train_unscaled=pca.transform(X_train)\n",
    "pca_std = std_clf.named_steps['pca']\n",
    "scaler = std_clf.named_steps['standardscaler']\n",
    "X_train_std = pca_std.transform(scaler.transform(X_train))\n",
    "# Show first principal componenets\n",
    "print('\\nPC 1 without scaling:\\n', pca.components_[0])\n",
    "print('\\nPC 1 with scaling:\\n', pca_std.components_[0])\n",
    "\n",
    "## t-SNE Reduce dim to 2D:\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(13,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y.astype(int), cmap=\"jet\")\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "## PCA and LocallyLinearEmbedding\n",
    "pca_lle = Pipeline([\n",
    "    (\"pca\", PCA(n_components=0.95, random_state=42)),\n",
    "    (\"lle\", LocallyLinearEmbedding(n_components=2, random_state=42)),\n",
    "])\n",
    "t0 = time.time()\n",
    "X_pca_lle_reduced = pca_lle.fit_transform(X)\n",
    "t1 = time.time()\n",
    "print(\"PCA+LLE took {:.1f}s.\".format(t1 - t0))\n",
    "plot_digits(X_pca_lle_reduced, y.astype(int))\n",
    "plt.show()\n",
    "\n",
    "## MDS\n",
    "X_mds_reduced = MDS(n_components=2, random_state=42).fit_transform(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8759cb53-2bb1-467a-a34c-3f849e8d3d7f",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457bf4ef-66c1-4ff7-bf9a-6cd8a5318659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports: \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## K-Means:\n",
    "km = KMeans(n_clusters=8, \n",
    "            init='random', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "km.score(X)\n",
    "PlotClusters(X,y_km, km)\n",
    "\n",
    "## For Loop Kmeans (Variation of the number of k-clusers):\n",
    "for n_clusters in range(4,9):\n",
    "    km = KMeans(n_clusters=n_clusters,\n",
    "                init='random', \n",
    "                n_init=10, \n",
    "                max_iter=300,\n",
    "                tol=1e-04,\n",
    "                random_state=0)\n",
    "    y_km = km.fit_predict(X)\n",
    "    PlotClusters(X,y_km, km)\n",
    "\n",
    "\n",
    "## Elbow Methods Plot: (k-means++) = Putting centroids far away\n",
    "distortions = []\n",
    "ScoreList   = []\n",
    "maxNumberOfClusters=15\n",
    "for i in range(1, maxNumberOfClusters):\n",
    "    km = KMeans(n_clusters=i, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                random_state=0)\n",
    "    km.fit(X)\n",
    "    distortions.append(km.inertia_)\n",
    "    ScoreList.append(-km.score(X))\n",
    "# Plot the ellbow plot:\n",
    "plt.plot(range(1, maxNumberOfClusters), distortions, marker='o')\n",
    "plt.plot(range(1, maxNumberOfClusters), ScoreList, marker='^')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "## Plot Clusters:\n",
    "from matplotlib import colors as mcolors\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "ColorNames=list(colors.keys())\n",
    "HSV=colors.values()\n",
    "def PlotClusters(X,y, km):\n",
    "    for ClusterNumber in range(km.n_clusters):\n",
    "        plt.scatter(X[y == ClusterNumber, 0],\n",
    "                X[y == ClusterNumber, 1],\n",
    "                s=50, c=ColorNames[ClusterNumber+1],\n",
    "                marker='s', edgecolor='black',\n",
    "                label='cluster {0}'.format(ClusterNumber+1))\n",
    "    plt.scatter(km.cluster_centers_[:, 0],\n",
    "        km.cluster_centers_[:, 1],\n",
    "        s=250, marker='*',\n",
    "        c='red', edgecolor='black',\n",
    "        label='centroids')\n",
    "    plt.legend(scatterpoints=1)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "## Kmeans on images:\n",
    "image_flattened = np.reshape(original_img, (width * height, depth))\n",
    "image_array_sample = shuffle(image_flattened, random_state=0)[:1000]\n",
    "estimator = KMeans(n_clusters=8, random_state=0)\n",
    "estimator.fit(image_array_sample)\n",
    "cluster_assignments = estimator.predict(image_flattened)\n",
    "\n",
    "## Spectral Clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
    "                          assign_labels='kmeans')\n",
    "\n",
    "labelsS = model.fit_predict(X_mn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25fded03-e154-48f0-a594-21699f14f8d4",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27fdce-92ce-49bb-92f8-5087d06804fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('stores.csv')\n",
    "features = data[['latitude', 'longitude']]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
    "dbscan.fit(scaled_features)\n",
    "labels = dbscan.labels_\n",
    "data['cluster_label'] = labels\n",
    "print(data)\n",
    "\n",
    "for eps in np.linspace(14,20,11):\n",
    "    print(\"\\neps={}\".format(eps))\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(X_pca)\n",
    "    print(\"Number of clusters: {}\".format(len(np.unique(labels))))\n",
    "    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4c8e3f1-a65f-40f5-aab2-60ff2f05da34",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac80ce8-d288-41b7-8dc6-44c29ce62db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_components = np.arange(1, 21)\n",
    "models = [mix.GaussianMixture(n, covariance_type='full',\n",
    "                             random_state=42).fit(X_train)\n",
    "         for n in n_components]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "ax.plot(n_components, [m.bic(X_train) for m in models], label='BIC')\n",
    "ax.plot(n_components, [m.aic(X_train) for m in models], label='AIC')\n",
    "ax.axvline(np.argmin([m.bic(X_train) for m in models]), color='blue')\n",
    "ax.axvline(np.argmin([m.aic(X_train) for m in models]), color='green')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af677634-3d59-482f-a2bd-f94b640eab98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f5212-e595-4e84-b1c0-fff72f230e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1ac45-4d42-4ac1-9f65-6c27e3d395ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294d9e8-8981-4c6a-9e68-604ffdf81d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b66e2b-91e9-43b0-93b5-268977fa0641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9ba65-901b-4f10-8adc-b637c69d5978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322c957-9ca7-496f-9259-1082dff4c111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd2175-3d4b-4852-828b-05c3a0d282de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aecda9-b4c9-4716-9143-e4310982cb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb36d6c-dbfe-4a60-9a8f-d93319208f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aabbb5-1618-49fe-a913-a7e982cb6382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fba21-eae0-44e4-a541-b8d5e86b3445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff342966-0d46-49d1-8147-9cf1d0847c0b",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9909a-a3e0-4c7a-ac95-f3d8167440b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Plot: Matplotlib\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.set(title=\"Calories\",xlabel=\"calories\",ylabel=\"index\")\n",
    "# Define (x,y) -> if not specified => Index\n",
    "ax.plot(df.Calories)\n",
    "plt.show()\n",
    "\n",
    "# Line plot with range:\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.set(title=\"Calories\", xlabel=\"Index\", ylabel=\"Calories\")\n",
    "x_values = np.arange(1, 101)  # Specify the desired range for the x-axis\n",
    "ax.plot(x_values, df[\"Calories\"].values[:100])\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot:\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.set(title=\"Scatterplot\",xlabel=\"Duration\",ylabel=\"Calories\")\n",
    "ax.scatter(df.Duration,df.Pulse, c=df.Pulse, cmap=\"coolwarm_r\")\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot with SNS\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.set(title=\"Scatterplot\",xlabel=\"Duration\",ylabel=\"Calories\")\n",
    "sns.scatterplot(df, x=\"Duration\",y=\"Pulse\", hue=df.Pulse)\n",
    "plt.show()\n",
    "\n",
    "# Vertical Barplot:\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.set(title=\"Barplot\",xlabel=\"Duration\",ylabel=\"Calories\")\n",
    "ax.bar(df.Duration,df.Pulse, color = 'darkblue')\n",
    "plt.show()\n",
    "\n",
    "# Horizontal Barplot:\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.set(title=\"Barplot\",xlabel=\"Duration\",ylabel=\"Calories\")\n",
    "ax.barh(df.Duration,df.Pulse, color = 'darkblue')\n",
    "plt.show()\n",
    "\n",
    "# Histograms:\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "ax.set(title=\"Histogram\",xlabel=\"WHatever\",ylabel=\"Frequency\")\n",
    "ax.hist(df.Pulse, color = 'darkblue',bins=50)\n",
    "plt.show()\n",
    "\n",
    "## Hard one: Barplot with hue (dependencies):\n",
    "df_CH_Drug = df.groupby([\"Drug\",\"Cholesterol\"]).size().reset_index(name = \"Count\")\n",
    "plt.figure(figsize = (9,5))\n",
    "sns.barplot(x = \"Drug\",y=\"Count\", hue = \"Cholesterol\",data = df_CH_Drug)\n",
    "plt.title(\"Cholesterol -- Drug\")\n",
    "plt.show()\n",
    "\n",
    "# Multiple plots: (1 row 3 columns)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18,5))\n",
    "i = 0\n",
    "for k in [1, 3, 5]: # 2 scatter plots in 1:\n",
    "  model = KNeighborsClassifier(n_neighbors=k).fit(X,y)\n",
    "  pred = model.predict(p[[\"x1\", \"x2\"]])\n",
    "  ax[i].scatter(data.x1, data.x2, c=data.y, cmap=\"coolwarm_r\")\n",
    "  ax[i].scatter(p.x1, p.x2, c=pred, cmap=\"coolwarm_r\", marker=\"x\")\n",
    "  ax[i].set_title(\"KNN with k = \" + str(k))\n",
    "  i += 1\n",
    "\n",
    "# Markers = o = circle . = point\n",
    "\n",
    "# Horizontal Line can be added by:\n",
    "ax.axvline(x=4, c=\"k\",  ls=\"--\")\n",
    "\n",
    "## Pairplot: Dependencies between target = Drug and other:\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.pairplot(df, hue=\"Drug\")\n",
    "\n",
    "## Countplot: Target and Categorical Input Features:\n",
    "df_CH_Drug = df.groupby([\"Drug\",\"Cholesterol\"]).size().reset_index(name = \"Count\")\n",
    "plt.figure(figsize = (9,5))\n",
    "sns.barplot(x = \"Drug\",y=\"Count\", hue = \"Cholesterol\",data = df_CH_Drug)\n",
    "plt.title(\"Cholesterol -- Drug\")\n",
    "plt.show()\n",
    "\n",
    "# Multiple plot:\n",
    "plt.figure()\n",
    "df.hist(figsize=(12,12),bins=37)\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix:\n",
    "corrmatrix=df.corr()\n",
    "import seaborn as sns\n",
    "corrmatrix\n",
    "sns.heatmap(corrmatrix)\n",
    "\n",
    "## Advanced correlation Matrix (absolute)\n",
    "# Get Feature names and column names\n",
    "features=df.iloc[:,:-1].columns;\n",
    "corr = df[features].corr()\n",
    "# Stack them and remove duplicate correlations\n",
    "features=df.iloc[:,:-1].columns;\n",
    "corr = df[features].corr()\n",
    "dcorr = corr.stack().reset_index()\n",
    "dcorr.columns = ['F1', 'F2', 'Corr']\n",
    "mask_dups = (dcorr[['F1', 'F2']].apply(frozenset, axis=1).duplicated()) | (dcorr['F1']==dcorr['F2'])\n",
    "dcorr = dcorr[~mask_dups]\n",
    "dcorr['Corr_abs'] = dcorr['Corr'].abs()\n",
    "# Highest absolute:\n",
    "dcorr.sort_values(by=['Corr_abs'], ascending=False).head(10)\n",
    "# Highest Positive:\n",
    "dcorr.sort_values(by=['Corr'], ascending=False).head(10)\n",
    "# Highest Negative:\n",
    "dcorr.sort_values(by=['Corr'], ascending=True).head(10)$\n",
    "# Plot it:\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "_ = sns.heatmap(corr, cmap=cmap, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "## KNN Scatter Plot (2 Classes):\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18,5))\n",
    "i = 0\n",
    "for k in [1, 3, 5]:\n",
    "  model = KNeighborsClassifier(n_neighbors=k).fit(X,y)\n",
    "  pred = model.predict(p[[\"x1\", \"x2\"]])\n",
    "  ax[i].scatter(data.x1, data.x2, c=data.y, cmap=\"coolwarm_r\")\n",
    "  ax[i].scatter(p.x1, p.x2, c=pred, cmap=\"coolwarm_r\", marker=\"x\")\n",
    "  ax[i].set_title(\"KNN with k = \" + str(k))\n",
    "  i += 1\n",
    "\n",
    "## Confusion Matrix:\n",
    "conf = confusion_matrix(y_test, pred)\n",
    "labels = []\n",
    "sns.heatmap(conf, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            cbar=False, \n",
    "            cmap=\"coolwarm_r\", \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels, \n",
    "            linewidth = 1)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "## ROC and Precision-Recall:\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Calculates Probabilities & Keep the Probabilities of the positive class only:\n",
    "probs = model.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# Function for plotting the ROC curve\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label = 'random classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curve & Compute the Area Under the ROC Curve (AUC) - the ROC AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "plot_roc_curve(fpr, tpr)\n",
    "auc = roc_auc_score(y_test, probs)\n",
    "print(\"AUC: \" , round(auc, 3))\n",
    "\n",
    "# Function for plotting the Precision-Recall curve\n",
    "def plot_rpc(recall, precision):\n",
    "    plt.plot(recall, precision, color='orange', label='RPC')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall = True Positive Rate')\n",
    "    plt.title('Recall-Precision Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot Precision-Recall curve  & Compute average precision - Precision-Recall AUC\n",
    "precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "plot_rpc(recall, precision)\n",
    "average_precision = average_precision_score(y_test, probs)\n",
    "print(\"Average Precision: \", round(average_precision, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c06f526-ebb7-43cd-8ae0-30f47d982b7f",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcfbea-a90e-4fc9-a90f-6b977d2f9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read a file:\n",
    "df=pf.read_csv(\"filepath.csv\")\n",
    "\n",
    "# Head or Tail:\n",
    "df.head(5), df.tail(5) \n",
    "\n",
    "# Summary of the data:\n",
    "df.info(), df.describe(), df.types\n",
    "\n",
    "# Column Names: \n",
    "df.columns\n",
    "\n",
    "# Shape of dataframe (rows, columns)\n",
    "df.shape\n",
    "df.shape[0]\n",
    "df.shape[1]\n",
    "\n",
    "# Select columns by name and index (0-3)\n",
    "cols = [\"CRASH DATE\", \"BOROUGH\", \"NUMBER OF PERSONS INJURED\"] \n",
    "df[cols]\n",
    "df.iloc[:,1:4]\n",
    "\n",
    "# Selecting rows (first 5)\n",
    "df[0:5]\n",
    "\n",
    "# Selecting rows and columns: row 1&4 and columns 0-2\n",
    "df.iloc[[1,4], 0:3]\n",
    "\n",
    "# Changing a value:\n",
    "df.loc[0, \"BOROUGH\"] = \"BROOKLYN\"\n",
    "df.iloc[0,1] = \"Brook\"\n",
    "\n",
    "# Boolean Indexing:\n",
    "boolean_condition = (df.LONGITUDE<-50) & (df.LONGITUDE>-74.5) & (df.LATITUDE< 41)\n",
    "df_filtered = df[boolean_condition]\n",
    "\n",
    "# Counting Number of Values in Column Frequency:\n",
    "df[\"BOROUGH\"].value_counts()\n",
    "df.BOROUGH.value_count()\n",
    "\n",
    "# Relative Frequency:\n",
    "df.Drug.value_counts(normalize=True)\n",
    "\n",
    "# Drop NaN values:\n",
    "df = pd.read_csv('data.csv')\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "# Fill NaN Values (All or column specific)\n",
    "df.fillna(130, inplace = True)\n",
    "df[\"Calories\"].fillna(130, inplace = True)\n",
    "\n",
    "# Fill NaN with the mean/median of the column:\n",
    "x = df[\"Calories\"].mean()\n",
    "x = df[\"Calories\"].median()\n",
    "df[\"Calories\"].fillna(x, inplace = True)\n",
    "\n",
    "# Convert to date:\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# For loop over rows to delete:\n",
    "for x in df.index:\n",
    "  if df.loc[x, \"Duration\"] > 120:\n",
    "    df.drop(x, inplace = True)\n",
    "\n",
    "# Duplicate rows:\n",
    "print(df.duplicated())\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "## Groupby:\n",
    "# Sum up the number of all injured persons per borough for all different boroughs\n",
    "df.groupby(\"BOROUGH\", as_index=False).[\"NUMBER OF PERSONS INJURED\"].sum()\n",
    "\n",
    "# Multiple Aggregation functions (sum & max):\n",
    "df.groupby(\"BOROUGH\", as_index=False)[\"NUMBER OF PERSONS INJURED\"].agg({\"SUM INJURED\": \"sum\", \"MAX INJURED\": \"max\"})\n",
    "\n",
    "# Apply an aggregating function on multiple variables\n",
    "df.groupby([\"BOROUGH\",\"VEHICLE TYPE CODE 1\"])[[\"NUMBER OF PERSONS INJURED\",\"CONTRIBUTING FACTOR VEHICLE 1\"]].sum()\n",
    "\n",
    "## Examples:\n",
    "# 1. Find the average age of the players for the year 2017.\n",
    "bool = (df.Year == 2017)\n",
    "df[bool].Age.mean()\n",
    "\n",
    "# 2. Plot the total number of points (`PTS`) per year since 2000\n",
    "bool = (df.Year >= 2000)\n",
    "df[bool].groupby(\"Year\",as_index=True).PTS.sum().plot(kind=\"barh\")\n",
    "\n",
    "# 3. Plot the number of players per year since 2010. \n",
    "bool = (df.Year >= 2010)\n",
    "df[bool].Year.value_counts(sort=True).plot(kind=\"bar\")\n",
    "\n",
    "## Direct Plotting inside of pandas:\n",
    "\n",
    "# Plots everything:\n",
    "df.plot()\n",
    "df.show()\n",
    "\n",
    "# Scatterplot (Duration / Calories)\n",
    "df.plot(kind = 'scatter', x = 'Duration', y = 'Calories')\n",
    "plt.show()\n",
    "\n",
    "# Histogram:\n",
    "df[\"NUMBER OF PERSONS INJURED\"].hist()\n",
    "df[\"NUMBER OF PERSONS INJURED\"].hist(bins=50)\n",
    "\n",
    "# Bar Charts (Vertical and Horizontal)\n",
    "df[\"BOROUGH\"].value_counts().plot(kind='bar')\n",
    "df.BOROUGH.value_counts().plot(kind='barh')\n",
    "\n",
    "# Line Plot:\n",
    "plot = (df['NUMBER OF PERSONS INJURED'].value_counts().plot( \n",
    "        kind='line', # we use a line plot because the x-axis is numeric/continuous\n",
    "        marker='o',  # we use a marker to mark where we have data points \n",
    "        logy=True # make the y-axis logarithmic\n",
    "))\n",
    "plot.set_xlabel(\"Number of injuries\")\n",
    "plot.set_ylabel(\"Number of collisions\")\n",
    "\n",
    "# KDE (Kernel Density Estimation)\n",
    "df[\"NUMBER OF PERSONS INJURED\"].plot(\n",
    "    kind='kde', \n",
    "    color='Black', \n",
    "    xlim=(0,5), \n",
    "    figsize=(15,5)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "849dd63c-c94c-4666-9217-af2228d39551",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e0148-bced-419a-862b-657f33a7b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en\n",
    "!pip install -U gensim\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "# Load English Model:\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "# Create document:\n",
    "my_doc = sp(text)\n",
    "my_doc\n",
    "# Tokenisation:\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "# POS Tags:\n",
    "for word in my_doc:\n",
    "    print(word.text, word.pos_)\n",
    "doc1 = sp(\"I like to fish\") # verb\n",
    "doc2 = sp(\"I eat a fish\") # noun\n",
    "for word in doc1:\n",
    "  print(word.text, word.pos_)\n",
    "for word in doc2:\n",
    "  print(word.text, word.pos_)\n",
    "\n",
    "# Split up into sentences:\n",
    "sents_list = []\n",
    "for sent in my_doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "\n",
    "# Remove Stop-words:\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stopwords: %d' % len(spacy_stopwords))\n",
    "print('20 stopwords: %s' % list(spacy_stopwords)[:20])\n",
    "filtered_sent = []\n",
    "for word in my_doc:\n",
    "    if word.is_stop == False:\n",
    "        filtered_sent.append(word.text)\n",
    "\n",
    "# Remove stop words & punctuation & spaces:\n",
    "filtered_sent2 = []\n",
    "removed_tokens = []\n",
    "for word in my_doc:\n",
    "  if (word.is_stop == True) or (word.is_punct == True) or (word.is_space == True):\n",
    "    removed_tokens.append(word.text)\n",
    "  else:\n",
    "    filtered_sent2.append(word.text)\n",
    "\n",
    "# Lemmatisation:\n",
    "lem = sp(\"run runs ran running runner runners\")\n",
    "for word in lem:\n",
    "    print(word.text, word.lemma_)\n",
    "\n",
    "## Text Representations:\n",
    "s1 = \"\"\"\n",
    "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies.\"\"\"\n",
    "s2 = \"\"\"Donald Trump is a great friend, and he has four or five Picassos on his plane. And that's where I would look at them.\"\"\" # from Shaquille O'Neal\n",
    "s3 = \"\"\"Donald Trump is a phony, a fraud. His promises are as worthless as a degree from Trump University.\"\"\" # from Mitt Romney\n",
    "texts = [s1, s2, s3]\n",
    "\n",
    "# Bag-of-words:\n",
    "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
    "bow = count.fit_transform(texts)\n",
    "# Show matrix:\n",
    "bow.toarray()\n",
    "# Get Feature names & Show as Dataframe:\n",
    "feature_names = count.get_feature_names_out()\n",
    "pd.DataFrame(\n",
    "    bow.todense(), \n",
    "    columns=feature_names\n",
    "    )\n",
    "\n",
    "# TF-IDF:\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# N-Grams (with TF-IDF bigram)\n",
    "tfidf = TfidfVectorizer(ngram_range=(2, 2), stop_words=[\"and\", \"a\", \"is\"])\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4006c659-c0f5-47b4-96c2-61fedab3e8e9",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff8eb3-34a1-486b-b5a7-8370db972468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, Activation, BatchNormalization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__) #version should be at least 1.15.x\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "# names of class labels (we have ten classes)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "train_images = train_images.reshape((len(train_images),28,28))\n",
    "test_images = test_images.reshape((len(test_images),28,28))\n",
    "\n",
    "# check the shapes of the training and test data \n",
    "print(\"shape for training (x) data : \", train_images.shape) # should be: 60'000 Images each with 28x28 pixels\n",
    "print(\"shape for training (y) data : \", train_labels.shape)  # 60'000 Labels with 10 classes\n",
    "print(\"shape for test (x) data     : \", test_images.shape)  # 10'000 Images with 28x28 pixels\n",
    "print(\"shape for test (y) data     : \", test_labels.shape)  # 10'000 Labels with 10 classes\n",
    "\n",
    "# to give you an overview of the data plot first 25 images with corresponding labels\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()\n",
    "\n",
    "model = Sequential()\n",
    "# data reshaped for Convolution2D\n",
    "train_images=train_images.reshape(60000,28,28,1)\n",
    "test_images=test_images.reshape(10000,28,28,1)\n",
    "model.add(Conv2D(filters = 32, kernel_size=(3,3), strides =1, padding='same', input_shape= (28,28,1), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(filters = 64, kernel_size=(4,4), strides =1, padding='same',activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Conv2D(filters = 128, kernel_size=(5,5), strides =1, padding='same',activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# final output softmax layer for 10 classes (do not modify this layer)\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# print a summary of your model\n",
    "model.summary()\n",
    "model.compile(optimizer = 'adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "log = model.fit(train_images, \n",
    "                train_labels, \n",
    "                batch_size=128,\n",
    "                epochs=10,\n",
    "                validation_split=0.1)\n",
    "plt.plot(log.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(log.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "test_scores = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])\n",
    "\n",
    "# you can also make predictions for the test data\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "max_probability_predictions = np.argmax(predictions, axis=1)\n",
    "conf_mat = confusion_matrix(test_labels, max_probability_predictions)\n",
    "conf_mat = ConfusionMatrixDisplay(conf_mat)\n",
    "conf_mat.plot()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
